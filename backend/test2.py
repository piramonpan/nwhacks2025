import os
import warnings
from langchain.llms import LlamaCpp
from langchain.chains import ConversationChain
from langchain.prompts import PromptTemplate
from langchain.memory import ConversationBufferMemory
from langchain.callbacks.base import BaseCallbackHandler

# ------------------------------------------------------------------------------
# 1. Optional: Environment and warnings setup
# ------------------------------------------------------------------------------
os.environ["PYTHONWARNINGS"] = os.getenv("PYTHONWARNINGS", "ignore")
warnings.filterwarnings("ignore", category=UserWarning)
warnings.filterwarnings("ignore", category=DeprecationWarning)


# ------------------------------------------------------------------------------
# 2. Roles / Prompt Templates
#    Adjust these to match your desired style for "debug" and "assistant" modes
# ------------------------------------------------------------------------------
roles_dict = {
    "debugging": (
        "You are a helpful debugging teacher. Don't give away the complete answer. "
        "Identify the line and file where the error occurs and explain it. "
        "Ask the user to correct it themselves. "
        "Current conversation:\n{history}\nHuman: {input}\n"
    ),
    "assistant": (
        "You are a helpful teacher assistant. Respond in a kind and encouraging manner. "
        "Do not directly give the complete solution but guide the user towards the answer. "
        "Current conversation:\n{history}\nHuman: {input}\n"
    )
}


# ------------------------------------------------------------------------------
# 3. FileStreamingCallbackHandler (Optional)
#    Writes tokens to a file as they are generated by the model
# ------------------------------------------------------------------------------
class FileStreamingCallbackHandler(BaseCallbackHandler):
    def __init__(self, file_path):
        self.file_path = file_path
        # Clear the file at the beginning
        with open(self.file_path, "w") as f:
            f.write("")

    def on_llm_new_token(self, token: str, **kwargs):
        with open(self.file_path, "a", encoding="utf-8") as f:
            f.write(token)
            f.flush()


# ------------------------------------------------------------------------------
# 4. LocalChat Class
#    - Initializes a local LlamaCpp model
#    - Creates two conversation chains:
#        a) debugging_chain (for debugging code)
#        b) general_chain   (for normal Q&A)
#    - Exposes methods for debugging a file and starting an interactive chat loop
# ------------------------------------------------------------------------------
class LocalChat:
    def __init__(
        self,
        model_path: str,
        output_file: str = "assistant_response.txt",
        temperature: float = 0.8,
        max_tokens: int = 2048,
        context_length: int = 2048,
    ):
        """
        Initialize the local LlamaCpp model, memory, and callback for streaming output.
        """

        # 4.1 Create a file-streaming callback (optional)
        self.file_streaming_callback = FileStreamingCallbackHandler(file_path=output_file)

        # 4.2 Conversation memory
        self.memory = ConversationBufferMemory()

        # 4.3 Initialize the LlamaCpp model
        self.llm = LlamaCpp(
            model_path=model_path,
            n_gpu_layers=-1,         # Adjust GPU layers if you have a GPU
            n_batch=1024,           # Adjust batch size based on your hardware
            temperature=temperature,
            max_tokens=max_tokens,
            context_length=context_length,
            n_ctx=4096,             # Some models can handle larger context windows
            streaming=True,
            callbacks=[self.file_streaming_callback]  # This streams output to file
        )

        # 4.4 Create ConversationChain objects for different roles
        debugging_prompt = PromptTemplate(
            template=roles_dict["debugging"],
            input_variables=["history", "input"],
        )
        assistant_prompt = PromptTemplate(
            template=roles_dict["assistant"],
            input_variables=["history", "input"],
        )

        self.debugging_chain = ConversationChain(
            llm=self.llm,
            memory=self.memory,
            prompt=debugging_prompt
        )
        self.assistant_chain = ConversationChain(
            llm=self.llm,
            memory=self.memory,
            prompt=assistant_prompt
        )

    def debug_code(self, file_path: str) -> str:
        """
        Reads code from a file and runs it through the debugging chain prompt.
        """
        try:
            with open(file_path, "r", encoding="utf-8") as f:
                error_content = f.read()
            query = f"Debug the following code:\n{error_content}"
            response = self.debugging_chain.run(input=query)
            return response
        except FileNotFoundError:
            return "Error: The specified file was not found."
        except Exception as e:
            return f"An error occurred while debugging the code: {e}"

    def assistant_reply(self, user_input: str) -> str:
        """
        Runs general (assistant) conversation chain.
        """
        return self.assistant_chain.run(input=user_input)

    def start_chat(self):
        """
        Start an interactive chat loop in the terminal.
        Type 'debug' to trigger debugging, or 'exit' to quit.
        """
        print("Chatbot: Hello! How can I assist you today?")
        print("  - Type 'debug' to debug a file,")
        print("  - Type 'exit' to quit,")
        print("  - Or ask me anything else!\n")

        while True:
            user_input = input("You: ")

            if user_input.lower() == "exit":
                print("Chatbot: Goodbye!")
                break

            elif user_input.lower() == "debug":
                # Example: Hardcode or ask the user for the file path
                file_path = "/Users/junhe/Desktop/nwhacks2025/backend/terminal_output_test_files.py/full_error.txt"
                response = self.debug_code(file_path)
            else:
                # General Q&A
                response = self.assistant_reply(user_input)

            # Since the model might output "Assistant:" text multiple times,
            # you can truncate after the first occurrence if needed.
            if response.count("Assistant:") > 1:
                response = response.split("Assistant:", 1)[0].strip()

            print(f"Chatbot: {response}")

            # Add user and AI messages to memory
            self.memory.chat_memory.add_user_message(user_input)
            self.memory.chat_memory.add_ai_message(response)


# ------------------------------------------------------------------------------
# 5. If run as the main script, start the chat
# ------------------------------------------------------------------------------
if __name__ == "__main__":
    # Adjust model_path to your local model
    local_model_path = "/Users/junhe/Desktop/nwhacks2025/backend/model/meta.gguf"

    # Initialize LocalChat with your local model
    chatbot = LocalChat(model_path=local_model_path)
    chatbot.start_chat()
